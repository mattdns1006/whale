%% Author_tex.tex
%% This file describes the coding for IET.cls

\documentclass{IET}%%%%where IET is the template name

%The authors can define any packages after the \documentclass{IET} command.

%Some of the packages are:
%\usepackage{hyperref} for linking the cross references
%\usepackage{graphics} for dealing with figures.
%\usepackage{algorithmic} for describing algorithms
\usepackage{fixltx2e}
\usepackage{hyperref}
\usepackage{capt-of}
%The author can find the documentation of the above style file and any additional
%supporting files if required from "http://www.ctan.org"

% *** Do not adjust lengths that control margins, column widths, etc. ***

\newtheorem{theorem}{Theorem}
\newtheorem{condition}{Condition}

\newcommand{\twoPic}{0.45}
\newcommand{\onePic}{2\twoPic}
\newcommand{\real}{\mathbb{R}}

\begin{document}

\title{A face classifer for North Atlantic Right whales}

\author[1,*]{Matt Smith}
\affil{Department of Statistics, University of Warwick, Coventry, United Kingdom}

\author{Abhir Bhalerao}
%%%% By default, the citations will come automatically,
%%%% The optional bracket "[2.*]" is used  to display the corresponding author symbol
\affil{Department of Computer Science, University of Warwick, Coventry, United Kingdom}

\author{Ben Graham}
\affil{Facebook Artificial Intelligence Research, Paris, France}
%%%% Corresponding author detail must placed here
\affil[*]{m.d.smith@warwick.ac.uk}

\abstract{Accurate monitoring of individuals in a threatened species is of upmost importance to conservationists and researchers. Human observation is expensive and autonomous ariel photography is becoming an increasingly useful technique regarding animal biometrics \cite{koh2012dawn,martin2012estimating}. Fewer than 500 North Atlantic right whales are left in the world's oceans. As with many animal biometric inspection processes, tracking and monitoring individuals is an extremely time consuming process. Advances in the implementation and performance of deep learning algorithms have drastically improved performance in object detection and recognition tasks \cite{lecun2015deep}.  We employ a wide range of interesting techniques to build a "face-identification" algorithm for ariel photos of 447 unique. We follow a conventional modern face recognition pipeline consisting of the stages: detect, align, represent and classify \cite{taigman2014deepface}. We use deep learning algorithms to both detect and classify. A fully convolutional network \cite{long2015fully} is employed to semantically segment a given image to detect the location of the whale's head and body, we then use PCA on the resulting image to normalize for the whale's direction. A significant amount of hand labelled masks are needed to generate enough supervised training data to make this work effectively \cite{hong2015decoupled}. We tackle this issue by employing semi-supervised learning techniques and histogram matching between images to improve our localization algorithm and find a significant improvement in our results.}

\maketitle

\section{Introduction}
We entered the 2015 Right Whale Recognition online competition issued by Kaggle. Data consists of aeriel images, the vast majority containing a single Right whale. There are $M$ = 447 unique whales, each of which has at least one photo in the training set which contains 4543 labelled images. The test set contains $N$ = 6925 unlabelled images. Evaluation is based on the multi-class logarithmic loss
\begin{eqnarray}
\text{logloss} = - \frac{1}{N} \sum_{i=1}^{N} \sum_{i=1}^{M} y_{ij} \text{log}(p_{ij}),
\end{eqnarray}
where log is the natural logarithm, $y_{ij}$ is 1 if observation $i$ belongs to whale $j$ and 0 otherwise, and $p_{ij}$ is the predicted probability that observation $i$ belongs to whale $j$. 

The data was collected and labelled over a 10 year period by NOAA (National Oceanic and Atmospehric Administration) scientists via numerous helicopter trips over the northern Atlantic. 

We follow the conventional pipeline of alignment and classification and break our task of classifying a given photo, denoted $X_i$ into two main stages, both of which employ using convolutional neural networks;
\begin{itemize}
\item Alignment - We first reduce the dimensionality of $X_i$ and normalize for the distance to and orientation of the whale, generating a headshot of the whale, denoted $X_i^h$.
\item Classification - $X_i^h$ is passed through a classifier which outputs a probability mass function over the 447 whales.
\end{itemize}

\subsection{Related work}
Subsection text here.

\section{Alignment}

It is helpful to remove variation in inputs before giving them to a deep learning algorithm and, especially with faces, the success of a learned network is highly dependant on an alignment step \cite{lecun2012efficient,taigman2014deepface}. 

\subsubsection{Mask prediction}

We randomly choose 550 images from our training set $M^{Train}$ and another random 150 images to generate a test set $M^{Test}$. Using a graphics editor, for each $X_i \in {M^{Train}, M^{Test}}$ we create a semantic mask denoted $M_i$. An example of a pair is shown in figure \ref{fig:whaleLabel}.
\begin{figure}[H]
\centering     %%% not \center
\subfigure[]{\includegraphics[width=\twoPic\textwidth]{images/maskExample/x.jpg}}
\subfigure[]{\includegraphics[width=\twoPic\textwidth]{images/maskExample/y.jpg}}
\caption{Example of $\{X_i,M_i\}$ pair;\
a) $X_i$, b) $M_i$.
}\label{fig:whaleLabel}
\end{figure}
We distinguish between head, body and sea using red, yellow and black respectivley. Having two colors for distinguishing parts of the whale enables us to infer the direcition in which the whale is pointing. We rescale each $X_i$ to dimension $w \times h \times c = 600 \times 900 \times 3$ and each $M_i$ to $w^{'}\times h^{'}\times c^{'} = 19 \times 29 \times 3$. We use a fully connected convolutional neural network (FCNN) to learn a function $f_1: \real^{w\times h \times c} \to \real^{w^{'}\times h^{'}\times c^{'}}$. We are not interested in a huge amount of detail being produced in our predicted $M_i$, enough to infer head and body location, hence our choice for relatively small output space.

We describe our neural network architecture as follows;
\begin{align}\label{eq:whaleconv1}
	 f_1 = \{\text{down\textsubscript{0}} - , ... , - \text{down\textsubscript{4}} - 3\text{C}_{2D}3/1 - \text{sigmoid}\}, 
\end{align}
	where 
\begin{align}
			 \text{conv\textsubscript{n}} &= \{(48 + 32\text{n})\text{C}_{2D}3/1 - \text{BN} - \text{ReLU}\}, \\	
     		 \text{down\textsubscript{n}}  &= \{\text{conv\textsubscript{n}}- \text{MP}3/2\}.
     		 \label{eq:whaleconv1block}
\end{align}
The following notation for denoting a network architecture is similar to \cite{graham2014fractional}. $\{a-b-c\}$ denotes a network where the intial tensor is passed through the three consecutive layers; $a$ to $b$ and finally $c$. $fC_{iD}k/s$ denotes an $i$ dimensional convolutional layer with kernel size $k$ in each dimension, a stride of $s$ and number of filters $f$. Similarly MP$k/s$ is a two dimensional max-pooling layer with kernel size $k$ and stride $s$. Other layer notations; BN $=$ batch normalization, ReLU and Sigmoid are layers of rectified linear activation units and sigmoid activation units respectively. An illustration of $f_1$ is given in figure \ref{fig:nn}.

\begin{figure}
\centering
\begin{minipage}[t]{.7\textwidth}
\centering
\vspace{0pt}
\centering
\includegraphics[width=\textwidth]{images/fcn/deconv2.png}
\label{fig:nn}
\caption{Overall architecture of $f_1$. Each sequential purple block represents the shape of the tensor after $down\textsubscript{i}$ as in \ref{eq:whaleconv1block}.}
\end{minipage}\hfill
\begin{minipage}[t]{.3\textwidth}
\centering
\vspace{40pt}
\begin{tabular}{cc|cc}
  No. & Size ($w\times h\times c$) &\\
    \hline
    \hline
    $1$ & $900 \times 600 \times 3$  & \\
    $2$ & $450 \times 300 \times 48$ &  \\
    $3$ & $225 \times 150 \times 80$ & \\ 
    $4$ & $113 \times 75 \times 112$ &\\
    $5$ & $38 \times 57 \times 176$ &\\
    $6$ &  $19 \times 29 \times 176$ &\\
    $7$ &  $19 \times 29 \times 3$ &\\
\end{tabular}
\captionof{table}{This is the caption for table}
\end{minipage}
\end{figure}
We train $f_1$ for 20 epochs, using the mean squared error (mse) as our loss function and the Adam optimization algorithm with an initial learning rate of $0.001$ and batch size of $5$. Together with the mse we also look at a variant of the S{\o}rensen-Dice coefficient to compare model performance \cite{sorensen1948method}
\begin{align}
\text{QS}(Y,\hat{Y})={\frac {2|Y\cap \hat{Y}| + 1}{|Y|+|\hat{Y}|+1}}.
\end{align}
To artificially increase the size of the dataset we perform random augmentation procedures to each image on the fly; including, flips, rotations, shifts and sheers. However It is well known that a large quantity of varied data is required to train deep networks well, especially to fit complicated functions and generalise well to unseen examples. It is not plausible to learn a huge number of parameters in deep networks reliably in this scenario \cite{zhu2011semi}. Our particular case we hand labelled 700 images, moreover, as can be seen in the particular case of figure \ref{fig:whaleLabel}, they are far from perfect. Large variations in brightness and sea color in $X_i$ ensures even more data is needed to generalize well. 

We combated variation in sea color and brightness by using a histogram matching \cite{gonzalez2002digital} based technique to normalize for such variation. Before each $X_i$ is fed into $f_1$ we match the cumulative distribution function (CDF) of each channel in its YUV color space to a prespecified baseline target image, denoted $T$. 

\begin{figure}
\centering
\begin{minipage}[t]{.5\textwidth}
\centering
\includegraphics[height=\textwidth]{images/histMatch/images1.png}
\caption{$X_i$, $T$ and histogram matched $X_i$.}
\end{minipage}\hfill
\begin{minipage}[t]{.5\textwidth}
\centering
\includegraphics[width=\textwidth]{images/histMatch/density.jpg}
\caption{Densities}
\end{minipage}
\end{figure}

\subsubsection{Mask processing}

To indicate

\section{Classification}

\subsubsection{Results}

\section{Conclusion}
Sample equations.

\section{Enunciations}
%%%% If the author wants to add or modify the enunciation style
%%%% they can define in the preamble as shown below.

%%%% \newtheoremstyle{theorem}{6pt}{6pt}{\rm}{}{\sffamily}{ }{ }{}
%%%% \theoremstyle{theorem}
%%%% \newtheorem{theorem}{\sc Theorem}[section]

%%%%\newtheoremstyle{corollary}{6pt}{6pt}{\rm}{}{\sffamily}{ }{ }{}
%%%%\theoremstyle{corollary}
%%%%\newtheorem{corollary}{\sc Corollary}[section]

%%%%\newtheoremstyle{definition}{6pt}{6pt}{\rm}{}{\sffamily}{ }{ }{}
%%%%\theoremstyle{definition}
%%%%\newtheorem{definition}[theorem]{\sc Definition}
%%%%
%%%%\newtheorem{exercise}[theorem]{Exercise}

%%\begin{theorem}\label{T0.1}
%%Assume that $\alpha>0, \gamma>1, \beta>\frac{\gamma+1}{\gamma-1}$.
%%Then there exists a small $\tau_1>0$, such that for $\tau\in
%%[0,\tau_1)$, if $c$ crosses $c(\tau)$ from the direction of
%%to  a small amplitude periodic traveling wave solution of
%%(2.1), and the period of $(\check{u}^p(s),\check{w}^p(s))$ is
%%\[
%%\check{T}(c)=c\cdot \left[\frac{2\pi}{\omega(\tau)}+O(c-c(\tau))\right].
%%\]
%%\end{theorem}
%%
%%
%%\begin{condition}\label{C2.2}
%%From (0.8) and (2.10), it holds
%%$\frac{d\omega}{d\tau}<0,\frac{dc}{d\tau}<0$ for $\tau\in
%%[0,\tau_1)$. This fact yields that the system (2.1) with delay
%%$\tau>0$ has the periodic traveling waves for smaller wave speed $c$
%%than that the system (2.1) with $\tau=0$ does. That is, the
%%delay perturbation stimulates an early occurrence of the traveling waves.
%%\end{condition}


\section{Figures \& Tables}

The output for figure is:

\begin{figure}[!h]
%\centering\includegraphics{figurename.eps}
%%%call your figure name in the place "figurename.eps"
\caption{Insert figure caption here
\subcaption{a}{Insert Sub caption here}
\subcaption{b}{Insert Sub caption here}}
\label{fig_sim}
\end{figure}


\vskip2pc

\noindent The output for table is:

\begin{table}[!h]
\processtable{An Example of a Table\label{table_example}}%%%Table caption goes here
{\tabcolsep24pt%%%%%%%%% \tabcolsep command is to adjust the inter column spacing of table
\begin{tabular}{|c||c|}%%%The number of columns has to be defined here
\hline
One & Two\\ %%%% Table body
\hline
Three & Four\\%%%% Table body
\hline
\end{tabular}}{}
\end{table}%%%End of the table

\section{Conclusion}
The conclusion text goes here.

\section{Acknowledgment}

Thanks to Christin Khan and Leah Crowe from NOAA for hand labeling the images. 
Kaggle for competition.

\bibliographystyle{IEEEtran}
\bibliography{mybib}
%\vbox{\subsection{Websites}}
%
%\bibitem{bib1}
%
%`Author Guide - IET Research Journals', http://digital-library.theiet.org/journals/author-guide, accessed 27
%November 2014
%
%\bibitem{bib2}
%`Research journal length policy', http://digital-library.theiet.org/files/research\_journals\_\break length\_policy.pdf, accessed 27
%November 2014
%
%\bibitem{bib3}
%`ORCID: Connecting research and researchers', http://orcid.org/, accessed 3 December 2014
%
%\bibitem{bib4}
%`Fundref', http://www.crossref.org/fundref/, accessed 4 December 2014
%
%\vbox{\subsection{Journal articles}}
%
%\bibitem{bib5}
%Smith, T., Jones, M.: 'The title of the paper', IET Syst. Biol., 2007, \textbf{1}, (2), pp. 1--7
%
%\bibitem{bib6}
%Borwn, L., Thomas, H., James, C.,~\textit{et al}.:'The title of the paper, IET
%Communications, 2012, \textbf{6}, (5), pp 125--138
%
%\vbox{\subsection{Conference Paper}}
%
%\bibitem{bib7}
%Jones, L., Brown, D.: 'The title of the conference paper'. Proc. Int.
%Conf. Systems Biology, Stockholm, Sweden, May 2006, pp. 1--7
%
%\vbox{\subsection{Book, book chapter and manual}}
%
%\bibitem{bib8}
%Hodges, A., Smith, N.: 'The title of the book chapter', in Brown, S.
%(Ed.): 'Handbook of Systems Biology' (IEE Press, 2004, 1st edn.), pp. 1--7
%
%\bibitem{bib9}
%Harrison, E.A., and Abbott, C.: 'The title of the book' (XYZ Press,
%2005, 2nd edn. 2006)
%
%\vbox{\subsection{Report}}
%
%\bibitem{bib10}
%IET., 'Report Title' (Publisher, 2013), pp. 1-5
%
%\vbox{\subsection{Patent}}
%
%\bibitem{bib11}
%Brown, F.: 'The title of the patent (if available)'. British Patent
%123456, July 2004
%
%\bibitem{bib12}
%Smith, D., Hodges, J.: British Patent Application 98765, 1925
%
%\vbox{\subsection{Thesis}}
%
%\bibitem{bib13}
%Abbott, N.L.: 'The title of the thesis'. PhD thesis, XYZ University, 2005
%
%\vbox{\subsection{Standard}}
%
%\bibitem{bib14}
%BS1234: 'The title of the standard', 2006
%
%
\section{Appendices}
%
Appendices are allowed but please be aware that these are included in the overall word count.

\end{document}
